{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll practice your knowledge of Ridge and Lasso regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will: \n",
    "\n",
    "- Use Lasso and Ridge regression with scikit-learn \n",
    "- Compare and contrast Lasso, Ridge and non-regularized regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Prices Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at yet another house pricing dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('Housing_Prices/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at `.info()` of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, split the data into `X` (predictor) and `y` (target) variables \n",
    "- Split the data into 75-25 training-test sets. Set the `random_state` to 10 \n",
    "- Remove all columns of `object` type from `X_train` and `X_test` and assign them to `X_train_cont` and `X_test_cont`, respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "y = df.SalePrice\n",
    "X = df.drop(['SalePrice', 'Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_by_type(dt, dtype):\n",
    "    not_dtype = dt.dtypes[dt.dtypes.values != dtype].index\n",
    "    return not_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=25, random_state=10)\n",
    "\n",
    "# Remove \"object\"-type features from X\n",
    "cont_features = remove_columns_by_type(X, 'object')\n",
    "\n",
    "# Remove \"object\"-type features from X_train and X_test\n",
    "X_train_cont = X_train[cont_features]\n",
    "X_test_cont = X_test[cont_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use this data to build a first naive linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill the missing values in data using median of the columns (use [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)) \n",
    "- Fit a linear regression model to this data \n",
    "- Compute the R-squared and the MSE for both the training and test sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values with median using SimpleImputer\n",
    "impute = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train_cont)\n",
    "X_train_imputed = pd.DataFrame(impute.transform(X_train_cont), columns=cont_features)\n",
    "X_test_imputed = pd.DataFrame(impute.transform(X_test_cont), columns=cont_features)\n",
    "\n",
    "# Fit the model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(observations, predictions):\n",
    "    mse = mean_squared_error(observations, predictions)\n",
    "    r2 = r2_score(observations, predictions)\n",
    "    print('- MSE:', mse)\n",
    "    print('- R-Squared:', r2)\n",
    "    print()\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 1173712195.5072715\n",
      "- R-Squared: 0.8131421927426098\n",
      "\n",
      "Test Set:\n",
      "- MSE: 1475142062.4466474\n",
      "- R-Squared: 0.8030090017771176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predictions for training & test sets\n",
    "y_pred_train = linreg.predict(X_train_imputed)\n",
    "y_pred_test = linreg.predict(X_test_imputed)\n",
    "\n",
    "# Print metrics for training set\n",
    "print(\"Training Set:\")\n",
    "print_metrics(y_train, y_pred_train)\n",
    "\n",
    "# Print metrics for test set\n",
    "print(\"Test Set:\")\n",
    "print_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalize your data using a `StandardScalar`  \n",
    "- Fit a linear regression model to this data \n",
    "- Compute the R-squared and the MSE for both the training and test sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 1173724960.128814\n",
      "- R-Squared: 0.8131401605841292\n",
      "\n",
      "Test Set:\n",
      "- MSE: 1474442758.7329516\n",
      "- R-Squared: 0.8031023870449701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the train and test data\n",
    "ss = StandardScaler().fit(X_train_imputed)\n",
    "X_train_imputed_scaled = pd.DataFrame(ss.transform(X_train_imputed), columns=cont_features)\n",
    "X_test_imputed_scaled = pd.DataFrame(ss.transform(X_test_imputed), columns=cont_features)\n",
    "\n",
    "# Fit the model\n",
    "linreg_norm = LinearRegression()\n",
    "linreg_norm.fit(X_train_imputed_scaled, y_train)\n",
    "\n",
    "# Generate predictions:\n",
    "y_pred_train_scaled = linreg_norm.predict(X_train_imputed_scaled)\n",
    "y_pred_test_scaled = linreg_norm.predict(X_test_imputed_scaled)\n",
    "\n",
    "# Print metrics for training set\n",
    "print(\"Training Set:\")\n",
    "print_metrics(y_train, y_pred_train_scaled)\n",
    "\n",
    "# Print metrics for test set\n",
    "print(\"Test Set:\")\n",
    "print_metrics(y_test, y_pred_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above models didn't include categorical variables so far, let's include them! \n",
    "\n",
    "\n",
    "- Include all columns of `object` type from `X_train` and `X_test` and assign them to `X_train_cat` and `X_test_cat`, respectively \n",
    "- Fill missing values in all these columns with the string `'missing'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_cat which contains only the categorical variables\n",
    "features_cat = list(set(X.columns) - set(cont_features))\n",
    "X_train_cat = X_train[features_cat]\n",
    "X_test_cat = X_test[features_cat]\n",
    "\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna('missing', inplace=True)\n",
    "X_test_cat.fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-hot encode all these categorical columns using `OneHotEncoder` \n",
    "- Transform the training and test DataFrames (`X_train_cat`) and (`X_test_cat`) \n",
    "- Run the given code to convert these transformed features into DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# OneHotEncode categorical variables fitted to training set\n",
    "ohe = OneHotEncoder(drop='first').fit(X_train_cat)\n",
    "\n",
    "# Transform both training & test sets:\n",
    "X_train_ohe = ohe.transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "# Convert these columns into a DataFrame\n",
    "columns = ohe.get_feature_names(X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine `X_train_imputed_scaled` and `cat_train_df` into a single DataFrame  \n",
    "- Similarly, combine `X_test_imputed_scaled` and `cat_test_df` into a single DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_train_all = pd.concat([X_train_imputed_scaled, cat_train_df], axis=1)\n",
    "X_test_all = pd.concat([X_test_imputed_scaled, cat_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a linear regression model using all the features (`X_train_all`). Also, print the R-squared and the MSE for both the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 415430938.3489134\n",
      "- R-Squared: 0.9338623944576052\n",
      "\n",
      "Test Set:\n",
      "- MSE: 846798095.9244462\n",
      "- R-Squared: 0.8869182796315077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "linreg_all = LinearRegression().fit(X_train_all, y_train)\n",
    "\n",
    "# Generate predictions for both training & test sets\n",
    "y_train_pred_all = linreg_all.predict(X_train_all)\n",
    "y_test_pred_all = linreg_all.predict(X_test_all)\n",
    "\n",
    "# print out metrics (MSE & R2) for both training & test sets\n",
    "# Training\n",
    "print(\"Training Set:\")\n",
    "linreg_metrics_train = print_metrics(y_train, y_train_pred_all)\n",
    "\n",
    "# Test\n",
    "print(\"Test Set:\")\n",
    "linreg_metrics_test = print_metrics(y_test, y_test_pred_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the severe overfitting above; our training R-squared is very high, but the test R-squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the data (normalized features and dummy categorical variables, `X_train_all`) to build two models - one each for Lasso and Ridge regression. Each time, look at R-squared and MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 416348842.23208976\n",
      "- R-Squared: 0.9337162619495339\n",
      "\n",
      "Test Set:\n",
      "- MSE: 846479932.0792089\n",
      "- R-Squared: 0.8869607673451099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "lasso_1 = Lasso()\n",
    "lasso_1.fit(X_train_all, y_train)\n",
    "\n",
    "y_pred_train_lasso = lasso_1.predict(X_train_all)\n",
    "y_pred_test_lasso = lasso_1.predict(X_test_all)\n",
    "\n",
    "# print out metrics (MSE & R2) for both training & test sets\n",
    "# Training\n",
    "print(\"Training Set:\")\n",
    "lasso_train_metrics_1 = print_metrics(y_train, y_pred_train_lasso)\n",
    "\n",
    "# Test\n",
    "print(\"Test Set:\")\n",
    "lasso_test_metrics_1 = print_metrics(y_test, y_pred_test_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With a higher regularization parameter (alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 449005105.0291533\n",
      "- R-Squared: 0.9285173062917178\n",
      "\n",
      "Test Set:\n",
      "- MSE: 858590507.9739647\n",
      "- R-Squared: 0.885343516711903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "lasso_10 = Lasso(alpha=10)\n",
    "lasso_10.fit(X_train_all, y_train)\n",
    "\n",
    "y_pred_train_lasso = lasso_10.predict(X_train_all)\n",
    "y_pred_test_lasso = lasso_10.predict(X_test_all)\n",
    "\n",
    "# print out metrics (MSE & R2) for both training & test sets\n",
    "# Training\n",
    "print(\"Training Set:\")\n",
    "lasso_train_metrics_10 = print_metrics(y_train, y_pred_train_lasso)\n",
    "\n",
    "# Test\n",
    "print(\"Test Set:\")\n",
    "lasso_test_metrics_10 = print_metrics(y_test, y_pred_test_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 538795298.1654778\n",
      "- R-Squared: 0.914222491372955\n",
      "\n",
      "Test Set:\n",
      "- MSE: 911378520.968107\n",
      "- R-Squared: 0.8782941865906594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "ridge_1 = Ridge()\n",
    "ridge_1.fit(X_train_all, y_train)\n",
    "\n",
    "y_pred_train_ridge = ridge_1.predict(X_train_all)\n",
    "y_pred_test_ridge = ridge_1.predict(X_test_all)\n",
    "\n",
    "# print out metrics (MSE & R2) for both training & test sets\n",
    "# Training\n",
    "print(\"Training Set:\")\n",
    "ridge_train_metrics_1 = print_metrics(y_train, y_pred_train_ridge)\n",
    "\n",
    "# Test\n",
    "print(\"Test Set:\")\n",
    "ridge_test_metrics_1 = print_metrics(y_test, y_pred_test_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With default parameter (alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "- MSE: 648599755.0834851\n",
      "- R-Squared: 0.8967413574754582\n",
      "\n",
      "Test Set:\n",
      "- MSE: 959213575.4773935\n",
      "- R-Squared: 0.8719062763156304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Your code here\n",
    "ridge_10 = Ridge(alpha=10)\n",
    "ridge_10.fit(X_train_all, y_train)\n",
    "\n",
    "y_pred_train_ridge = ridge_10.predict(X_train_all)\n",
    "y_pred_test_ridge = ridge_10.predict(X_test_all)\n",
    "\n",
    "# print out metrics (MSE & R2) for both training & test sets\n",
    "# Training\n",
    "print(\"Training Set:\")\n",
    "ridge_train_metrics_10 = print_metrics(y_train, y_pred_train_ridge)\n",
    "\n",
    "# Test\n",
    "print(\"Test Set:\")\n",
    "ridge_test_metrics_10 = print_metrics(y_test, y_pred_test_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the metrics    \n",
    "\n",
    "Write your conclusions here: \n",
    "_________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression\talpha\t\tSet\t\tMSE\t\t\tR-Squared\n",
      "Linear\t\tn/a\t\tTraining\t415430938.3489134\t0.9338623944576052\n",
      "Linear\t\tn/a\t\tTest\t\t846798095.9244462\t0.8869182796315077\n",
      "\n",
      "Lasso\t\t1\t\tTraining\t416348842.23208976\t0.9337162619495339\n",
      "Lasso\t\t1\t\tTest\t\t846479932.0792089\t0.8869607673451099\n",
      "\n",
      "Lasso\t\t10\t\tTraining\t449005105.0291533\t0.9285173062917178\n",
      "Lasso\t\t10\t\tTest\t\t858590507.9739647\t0.885343516711903\n",
      "\n",
      "Ridge\t\t1\t\tTraining\t538795298.1654778\t0.914222491372955\n",
      "Ridge\t\t1\t\tTest\t\t911378520.968107\t0.8782941865906594\n",
      "\n",
      "Ridge\t\t10\t\tTraining\t648599755.0834851\t0.8967413574754582\n",
      "Ridge\t\t10\t\tTest\t\t959213575.4773935\t0.8719062763156304\n"
     ]
    }
   ],
   "source": [
    "print('Regression\\talpha\\t\\tSet\\t\\tMSE\\t\\t\\tR-Squared')\n",
    "print(f'Linear\\t\\tn/a\\t\\tTraining\\t{linreg_metrics_train[0]}\\t{linreg_metrics_train[1]}')\n",
    "print(f'Linear\\t\\tn/a\\t\\tTest\\t\\t{linreg_metrics_test[0]}\\t{linreg_metrics_test[1]}')\n",
    "print()\n",
    "print(f'Lasso\\t\\t1\\t\\tTraining\\t{lasso_train_metrics_1[0]}\\t{lasso_train_metrics_1[1]}')\n",
    "print(f'Lasso\\t\\t1\\t\\tTest\\t\\t{lasso_test_metrics_1[0]}\\t{lasso_test_metrics_1[1]}')\n",
    "print()\n",
    "print(f'Lasso\\t\\t10\\t\\tTraining\\t{lasso_train_metrics_10[0]}\\t{lasso_train_metrics_10[1]}')\n",
    "print(f'Lasso\\t\\t10\\t\\tTest\\t\\t{lasso_test_metrics_10[0]}\\t{lasso_test_metrics_10[1]}')\n",
    "print()\n",
    "print(f'Ridge\\t\\t1\\t\\tTraining\\t{ridge_train_metrics_1[0]}\\t{ridge_train_metrics_1[1]}')\n",
    "print(f'Ridge\\t\\t1\\t\\tTest\\t\\t{ridge_test_metrics_1[0]}\\t{ridge_test_metrics_1[1]}')\n",
    "print()\n",
    "print(f'Ridge\\t\\t10\\t\\tTraining\\t{ridge_train_metrics_10[0]}\\t{ridge_train_metrics_10[1]}')\n",
    "print(f'Ridge\\t\\t10\\t\\tTest\\t\\t{ridge_test_metrics_10[0]}\\t{ridge_test_metrics_10[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between Ridge and Lasso, Lasso seems to be performing better with a test R-Squared of 0.8869607673451099 at alpha = 1, compared to the Ridge's R-Squared of 0.8782941865906594 at the same alpha. \n",
    "\n",
    "However, if we're using Linear Regression model without regularization as baseline, only R-squared of Lasso at alpha = 1 is higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare number of parameter estimates that are (very close to) 0 for Ridge and Lasso\n",
    "\n",
    "Use 10**(-10) as an estimate that is very close to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 10**(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1:\n",
      "0\n",
      "alpha = 10:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Number of Ridge params almost zero\n",
    "print('alpha = 1:')\n",
    "print(sum(np.abs(ridge_1.coef_) < thresh))\n",
    "\n",
    "print('alpha = 10:')\n",
    "print(sum(np.abs(ridge_10.coef_) < thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1:\n",
      "8\n",
      "alpha = 10:\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Number of Lasso params almost zero\n",
    "print('alpha = 1:')\n",
    "print(sum(np.abs(lasso_1.coef_) < thresh))\n",
    "\n",
    "print('alpha = 10:')\n",
    "print(sum(np.abs(lasso_10.coef_) < thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "0.18773946360153257\n"
     ]
    }
   ],
   "source": [
    "print(len(lasso_10.coef_))\n",
    "print(sum(abs(lasso_10.coef_) < 10**(-10)) / len(lasso_10.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso was very effective to essentially perform variable selection and remove about 25% of the variables from your model!\n",
    "\n",
    "--> My version of Lasso only removes around 18% of the variables\n",
    "\n",
    "Overall my models generate slightly different metrics compared to the solution. This could be due to:\n",
    "- The solution keeps `'Id'` column in X, which I do not think is the accurate way to handle 'ID' columns in general.\n",
    "- OneHotEncoder of the solution does not drop one of the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring all of our work together lets take a moment to put all of our preprocessing steps for categorical and continuous variables into one function. This function should take in our features as a dataframe `X` and target as a Series `y` and return a training and test DataFrames with all of our preprocessed features along with training and test targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    '''Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n",
    "    train and test DataFrames with targets'''\n",
    "    \n",
    "    # Train-test split (75-25), set seed to 10\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "    \n",
    "    \n",
    "    # Remove \"object\"-type features and SalesPrice from X\n",
    "    cont_features = remove_columns_by_type(X, 'object')\n",
    "    # Remove \"object\"-type features from X_train and X_test\n",
    "    X_train_cont = X_train[cont_features]\n",
    "    X_test_cont = X_test[cont_features]\n",
    "\n",
    "    \n",
    "    # Impute missing values with median using SimpleImputer\n",
    "    impute = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train_cont)\n",
    "    X_train_imputed = pd.DataFrame(impute.transform(X_train_cont), columns=cont_features)\n",
    "    X_test_imputed = pd.DataFrame(impute.transform(X_test_cont), columns=cont_features)\n",
    "\n",
    "\n",
    "    # Scale the train and test data\n",
    "    ss = StandardScaler().fit(X_train_imputed)\n",
    "    X_train_imputed_scaled = pd.DataFrame(ss.transform(X_train_imputed), columns=cont_features)\n",
    "    X_test_imputed_scaled = pd.DataFrame(ss.transform(X_test_imputed), columns=cont_features)\n",
    "\n",
    "\n",
    "    # Create X_cat which contains only the categorical variables\n",
    "    features_cat = list(set(X.columns) - set(cont_features))\n",
    "    X_train_cat = X_train[features_cat]\n",
    "    X_test_cat = X_test[features_cat]\n",
    "\n",
    "\n",
    "    # Fill nans with a value indicating that that it is missing\n",
    "    X_train_cat.fillna('missing', inplace=True)\n",
    "    X_test_cat.fillna('missing', inplace=True)\n",
    "\n",
    "\n",
    "    # OneHotEncode Categorical variables\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore').fit(X_train_cat)\n",
    "    \n",
    "    # Transform both training & test sets:\n",
    "    X_train_ohe = ohe.transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "    \n",
    "    # Convert these columns into a DataFrame\n",
    "    columns = ohe.get_feature_names(X_train_cat.columns)\n",
    "    cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "    cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "\n",
    "    \n",
    "    # Combine categorical and continuous features into the final dataframe\n",
    "    X_train_all = pd.concat([X_train_imputed_scaled, cat_train_df], axis=1)\n",
    "    X_test_all = pd.concat([X_test_imputed_scaled, cat_test_df], axis=1)\n",
    "    \n",
    "    return X_train_all, X_test_all, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the training and test error to find optimal alpha values\n",
    "\n",
    "Earlier we tested two values of alpha to see how it effected our MSE and the value of our coefficients. We could continue to guess values of alpha for our Ridge or Lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alphas = []\n",
    "\n",
    "for alpha in np.linspace(0, 200, num=50):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_all, y_train)\n",
    "    \n",
    "    train_preds = lasso.predict(X_train_all)\n",
    "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
    "    \n",
    "    test_preds = lasso.predict(X_test_all)\n",
    "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
    "    \n",
    "    alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha Value: 40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, train_mse, label='Train')\n",
    "ax.plot(alphas, test_mse, label='Test')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "# np.argmin() returns the index of the minimum value in a list\n",
    "optimal_alpha = alphas[np.argmin(test_mse)]\n",
    "\n",
    "# Add a vertical line where the test MSE is minimized\n",
    "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
    "ax.legend();\n",
    "\n",
    "print(f'Optimal Alpha Value: {int(optimal_alpha)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at this graph of our training and test MSE against alpha. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what alpha represents and how it relates to overfitting vs underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! You now know how to build Lasso and Ridge regression models, use them for feature selection and find an optimal value for $\\text{alpha}$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
